{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-02T11:20:52.464942Z","iopub.status.busy":"2024-10-02T11:20:52.464484Z","iopub.status.idle":"2024-10-02T11:20:55.347371Z","shell.execute_reply":"2024-10-02T11:20:55.346467Z","shell.execute_reply.started":"2024-10-02T11:20:52.464903Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing Generator...\n","Generator output shape: (1, 128, 128, 3)\n","Testing Discriminator...\n","Discriminator output shape: (1, 32, 32, 1)\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"g_net\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"g_net\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)          │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,832</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)        │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)        │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,120</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ downsample_block                │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,240</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DownsampleBlock</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)        │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,968</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)        │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,048</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ downsample_block_1              │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,936</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DownsampleBlock</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)  │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">201,984</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_1                │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">201,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_2                │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">201,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_3                │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">201,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ upsample_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpsampleBlock</span>)  │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,424</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)       │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,968</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ upsample_block_1                │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpsampleBlock</span>)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvBlock</span>)       │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,120</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,411</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv_block (\u001b[38;5;33mConvBlock\u001b[0m)          │ ?                      │         \u001b[38;5;34m4,832\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_1 (\u001b[38;5;33mConvBlock\u001b[0m)        │ ?                      │        \u001b[38;5;34m18,688\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_2 (\u001b[38;5;33mConvBlock\u001b[0m)        │ ?                      │        \u001b[38;5;34m37,120\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ downsample_block                │ ?                      │        \u001b[38;5;34m74,240\u001b[0m │\n","│ (\u001b[38;5;33mDownsampleBlock\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_4 (\u001b[38;5;33mConvBlock\u001b[0m)        │ ?                      │       \u001b[38;5;34m147,968\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_5 (\u001b[38;5;33mConvBlock\u001b[0m)        │ ?                      │        \u001b[38;5;34m18,048\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ downsample_block_1              │ ?                      │       \u001b[38;5;34m295,936\u001b[0m │\n","│ (\u001b[38;5;33mDownsampleBlock\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block (\u001b[38;5;33mResidualBlock\u001b[0m)  │ ?                      │       \u001b[38;5;34m201,984\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_1                │ ?                      │       \u001b[38;5;34m201,984\u001b[0m │\n","│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_2                │ ?                      │       \u001b[38;5;34m201,984\u001b[0m │\n","│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ residual_block_3                │ ?                      │       \u001b[38;5;34m201,984\u001b[0m │\n","│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ upsample_block (\u001b[38;5;33mUpsampleBlock\u001b[0m)  │ ?                      │       \u001b[38;5;34m295,424\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_16 (\u001b[38;5;33mConvBlock\u001b[0m)       │ ?                      │       \u001b[38;5;34m147,968\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ upsample_block_1                │ ?                      │        \u001b[38;5;34m73,984\u001b[0m │\n","│ (\u001b[38;5;33mUpsampleBlock\u001b[0m)                 │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv_block_18 (\u001b[38;5;33mConvBlock\u001b[0m)       │ ?                      │        \u001b[38;5;34m37,120\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │         \u001b[38;5;34m9,411\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,968,675</span> (7.51 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,968,675\u001b[0m (7.51 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,960,163</span> (7.48 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,960,163\u001b[0m (7.48 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,512</span> (33.25 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,512\u001b[0m (33.25 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"d_net\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"d_net\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">391,489</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ ?                      │       \u001b[38;5;34m391,489\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,489</span> (1.49 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m391,489\u001b[0m (1.49 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,489</span> (1.49 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m391,489\u001b[0m (1.49 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["\n","import os\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers, initializers, regularizers, optimizers, models\n","from glob import glob\n","from PIL import Image\n","from tensorflow.keras.applications import VGG19\n","from tensorflow.keras.models import Model\n","\n","# -------------------------------\n","# Utilities\n","# -------------------------------\n","from tensorflow.keras.layers import Layer\n","\n","class InstanceNormalization(Layer):\n","    def __init__(self, epsilon=1e-5, **kwargs):\n","        super(InstanceNormalization, self).__init__(**kwargs)\n","        self.epsilon = epsilon\n","\n","    def build(self, input_shape):\n","        # Khởi tạo các tham số scale và offset\n","        self.scale = self.add_weight(name='scale',\n","                                     shape=(input_shape[-1],),\n","                                     initializer='ones',\n","                                     trainable=True)\n","        self.offset = self.add_weight(name='offset',\n","                                      shape=(input_shape[-1],),\n","                                      initializer='zeros',\n","                                      trainable=True)\n","        super(InstanceNormalization, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # Tính toán trung bình và độ lệch chuẩn theo từng instance và channel\n","        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n","        inv = tf.math.rsqrt(variance + self.epsilon)\n","        normalized = (inputs - mean) * inv\n","        return self.scale * normalized + self.offset\n","\n","    def get_config(self):\n","        config = super(InstanceNormalization, self).get_config()\n","        config.update({\"epsilon\": self.epsilon})\n","        return config\n","\n","def check_folder(path):\n","    \"\"\"Ensure that a directory exists; create it if it doesn't.\"\"\"\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def show_all_variables(model):\n","    \"\"\"Print all trainable variables.\"\"\"\n","    trainable_vars = model.trainable_variables  # Lấy biến có thể huấn luyện từ model\n","    for var in trainable_vars:\n","        print(var.name, var.shape)\n","\n","\n","# -------------------------------\n","# Custom Layers and Functions\n","# -------------------------------\n","\n","\n","\n","\n","def rgb2yuv(rgb):\n","    \"\"\"Convert RGB image to YUV color space.\"\"\"\n","    yuv = tf.image.rgb_to_yuv((rgb + 1.0) / 2.0)  # Assuming input is in [-1, 1]\n","    return yuv\n","\n","\n","class DiscriminatorLossLayer(tf.keras.layers.Layer):\n","    def __init__(self, loss_type, **kwargs):\n","        super(DiscriminatorLossLayer, self).__init__(**kwargs)\n","        self.loss_type = loss_type\n","\n","    def call(self, inputs):\n","        \"\"\"Compute discriminator loss based on GAN type.\"\"\"\n","        real, gray, fake, real_blur = inputs\n","        if self.loss_type in ['wgan-gp', 'wgan-lp']:\n","            real_loss = -tf.reduce_mean(real)\n","            gray_loss = tf.reduce_mean(gray)\n","            fake_loss = tf.reduce_mean(fake)\n","            real_blur_loss = tf.reduce_mean(real_blur)\n","        elif self.loss_type == 'lsgan':\n","            real_loss = tf.reduce_mean(tf.square(real - 1.0))\n","            gray_loss = tf.reduce_mean(tf.square(gray))\n","            fake_loss = tf.reduce_mean(tf.square(fake))\n","            real_blur_loss = tf.reduce_mean(tf.square(real_blur))\n","        elif self.loss_type in ['gan', 'dragan']:\n","            real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n","            gray_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(gray), logits=gray))\n","            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n","            real_blur_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(real_blur), logits=real_blur))\n","        elif self.loss_type == 'hinge':\n","            real_loss = tf.reduce_mean(tf.nn.relu(1.0 - real))\n","            gray_loss = tf.reduce_mean(tf.nn.relu(1.0 + gray))\n","            fake_loss = tf.reduce_mean(tf.nn.relu(1.0 + fake))\n","            real_blur_loss = tf.reduce_mean(tf.nn.relu(1.0 + real_blur))\n","        else:\n","            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n","\n","        # Assign weights based on your specific requirements\n","        weights = {\n","            'real_loss': 1.7,\n","            'fake_loss': 1.7,\n","            'gray_loss': 1.7,\n","            'real_blur_loss': 1.0\n","        }\n","\n","        # Compute the total loss\n","        loss = (weights['real_loss'] * real_loss +\n","                weights['fake_loss'] * fake_loss +\n","                weights['gray_loss'] * gray_loss +\n","                weights['real_blur_loss'] * real_blur_loss)\n","        return loss\n","\n","class GeneratorLossLayer(Layer):\n","    def __init__(self, loss_type='gan', **kwargs):\n","        super(GeneratorLossLayer, self).__init__(**kwargs)\n","        self.loss_type = loss_type\n","    def call(self, fake):\n","        if self.loss_type in ['wgan-gp', 'wgan-lp']:\n","            fake_loss = -tf.reduce_mean(fake)\n","        elif self.loss_type == 'lsgan':\n","            fake_loss = tf.reduce_mean(tf.square(fake - 1.0))\n","        elif self.loss_type in ['gan', 'dragan']:\n","            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n","        elif self.loss_type == 'hinge':\n","            fake_loss = -tf.reduce_mean(fake)\n","        else:\n","            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n","\n","        return fake_loss\n","\n","class ContentLossLayer(layers.Layer):\n","    def call(self, real_features, generated_features):\n","        return tf.reduce_mean(tf.square(real_features - generated_features))\n","\n","class StyleLossLayer(layers.Layer):\n","    def call(self, real_features, generated_features):\n","        def gram_matrix(features):\n","            shape = tf.shape(features)\n","            reshaped_features = tf.reshape(features, [shape[0], -1, shape[-1]])\n","            gram = tf.matmul(reshaped_features, reshaped_features, transpose_a=True)\n","            return gram / tf.cast(shape[1] * shape[2], tf.float32)\n","        \n","        return tf.reduce_mean(tf.square(gram_matrix(real_features) - gram_matrix(generated_features)))\n","\n","class TotalVariationLossLayer(layers.Layer):\n","    def call(self, generated_image):\n","        return tf.reduce_sum(tf.image.total_variation(generated_image))\n","class RgbToYuvLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(RgbToYuvLayer, self).__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        \"\"\"Convert RGB to YUV.\"\"\"\n","        yuv = tf.image.rgb_to_yuv((inputs + 1.0) / 2.0)  # Assuming input is in [-1, 1]\n","        return yuv\n","class L1LossLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(L1LossLayer, self).__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        x, y = inputs\n","        return tf.reduce_mean(tf.abs(x - y))\n","class HuberLossLayer(Layer):\n","    def __init__(self, delta=1.0, **kwargs):\n","        super(HuberLossLayer, self).__init__(**kwargs)\n","        self.delta = delta\n","\n","    def call(self, inputs):\n","        y_true, y_pred = inputs\n","        huber_loss_fn = tf.keras.losses.Huber()\n","        return huber_loss_fn(y_true, y_pred)\n","def color_loss_fn(real, fake):\n","    real_yuv = rgb2yuv(real)\n","    fake_yuv = rgb2yuv(fake)\n","    \n","    # Call the layer as a function to compute the actual loss values\n","    y_loss = HuberLossLayer()([real_yuv[..., 0], fake_yuv[..., 0]])\n","    u_loss = HuberLossLayer()([real_yuv[..., 1], fake_yuv[..., 1]])\n","    v_loss = HuberLossLayer()([real_yuv[..., 2], fake_yuv[..., 2]])\n","    \n","    return y_loss + u_loss + v_loss\n","\n","\n","# -------------------------------\n","# Data Loader\n","# -------------------------------\n","import numpy as np\n","import cv2\n","import os\n","from glob import glob\n","\n","\n","class ImageGenerator:\n","    def __init__(self, dataset_path, img_size, batch_size):\n","        self.dataset_path = dataset_path\n","        self.img_size = img_size\n","        self.batch_size = batch_size\n","        self.image_paths = glob(os.path.join(dataset_path, '*.*'))\n","        self.num_images = len(self.image_paths)\n","        self.pointer = 0\n","\n","    def load_images(self):\n","        \"\"\"Load a batch of images (both types).\"\"\"\n","        batch_images = []\n","        for _ in range(self.batch_size):\n","            if self.pointer >= self.num_images:\n","                self.pointer = 0\n","                np.random.shuffle(self.image_paths)\n","            img, img_gray = self.load_image(self.image_paths[self.pointer])\n","            batch_images.append((img, img_gray))  # Thêm ảnh màu và ảnh xám\n","            self.pointer += 1\n","        return np.array(batch_images)\n","\n","    def load_image(self, image_path):\n","        \"\"\"Load and preprocess a single image (color and grayscale).\"\"\"\n","        img = cv2.imread(image_path)\n","        if img is None:\n","            raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n","        \n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (self.img_size, self.img_size))\n","        \n","        # Normalize image to [-1, 1]\n","        img = img.astype(np.float32) / 127.5 - 1\n","        \n","        # Create grayscale image from color image\n","        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        img_gray = cv2.resize(img_gray, (self.img_size, self.img_size))\n","        img_gray = np.stack([img_gray] * 3, axis=-1)  # Convert to 3 channels\n","        img_gray = img_gray.astype(np.float32) / 127.5 - 1  # Normalize grayscale image to [-1, 1]\n","        \n","        return img, img_gray\n","\n","    def save_images(self, images, image_path, photo_path=None):\n","        fake = self.inverse_transform(images.squeeze())\n","        if photo_path:\n","            return self.imsave(self.adjust_brightness_from_src_to_dst(fake, self.read_img(photo_path)[0]), image_path)\n","        else:\n","            return self.imsave(fake, image_path)\n","\n","    def inverse_transform(self, images):\n","        images = (images + 1.) / 2 * 255\n","        images = np.clip(images, 0, 255)\n","        return images.astype(np.uint8)\n","\n","    def imsave(self, images, path):\n","        return cv2.imwrite(path, cv2.cvtColor(images, cv2.COLOR_RGB2BGR))\n","\n","    def read_img(self, image_path):\n","        img = cv2.imread(image_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        img_gray = np.stack([img_gray] * 3, axis=-1)  # Chuyển đổi về 3 kênh\n","        return img, img_gray\n","\n","    def calculate_average_brightness(self, img):\n","        R = img[..., 0].mean()\n","        G = img[..., 1].mean()\n","        B = img[..., 2].mean()\n","        brightness = 0.299 * R + 0.587 * G + 0.114 * B\n","        return brightness, B, G, R\n","\n","    def adjust_brightness_from_src_to_dst(self, dst, src, path=None, if_show=None, if_info=None):\n","        brightness1, B1, G1, R1 = self.calculate_average_brightness(src)\n","        brightness2, B2, G2, R2 = self.calculate_average_brightness(dst)\n","        brightness_difference = brightness1 / brightness2\n","\n","        if if_info:\n","            print('Average brightness of source image:', brightness1)\n","            print('Average brightness of target image:', brightness2)\n","            print('Brightness difference:', brightness_difference)\n","\n","        dstf = dst * brightness_difference\n","        dstf = np.clip(dstf, 0, 255).astype(np.uint8)\n","\n","        ma, na, _ = src.shape\n","        mb, nb, _ = dst.shape\n","        result_show_img = np.zeros((max(ma, mb), 3 * max(na, nb), 3), dtype=np.uint8)\n","        result_show_img[:mb, :nb, :] = dst\n","        result_show_img[:ma, nb:nb + na, :] = src\n","        result_show_img[:mb, nb + na:nb + na + nb, :] = dstf\n","\n","        if if_show:\n","            cv2.imshow('Brightness Adjustment', cv2.cvtColor(result_show_img, cv2.COLOR_BGR2RGB))\n","            cv2.waitKey(0)\n","            cv2.destroyAllWindows()\n","\n","        if path is not None:\n","            cv2.imwrite(path, cv2.cvtColor(result_show_img, cv2.COLOR_RGB2BGR))\n","\n","        return dstf\n","\n","\n","\n","\n","# -------------------------------\n","# Models\n","# -------------------------------\n","\n","# Simplified ConvBlock: Convolution followed by Normalization and Activation\n","class ConvBlock(layers.Layer):\n","    def __init__(self, filters, kernel_size=3, strides=1, use_depthwise=False, **kwargs):\n","        super(ConvBlock, self).__init__(**kwargs)\n","        if use_depthwise:\n","            self.conv = layers.SeparableConv2D(filters, kernel_size, strides=strides, padding='same', use_bias=False)\n","        else:\n","            self.conv = layers.Conv2D(filters, kernel_size, strides=strides, padding='same', use_bias=False)\n","        self.norm = layers.BatchNormalization()\n","        self.activation = layers.LeakyReLU(0.2)\n","    \n","    def call(self, inputs, training=False):\n","        x = self.conv(inputs)\n","        x = self.norm(x, training=training)\n","        return self.activation(x)\n","\n","# Simplified Residual Block with optional depthwise separable convolution\n","class ResidualBlock(layers.Layer):\n","    def __init__(self, filters, use_depthwise=False, **kwargs):\n","        super(ResidualBlock, self).__init__(**kwargs)\n","        self.conv1 = ConvBlock(filters, kernel_size=1, strides=1)\n","        self.conv2 = ConvBlock(filters, kernel_size=3, strides=1, use_depthwise=use_depthwise)\n","        self.conv3 = layers.Conv2D(filters, kernel_size=1, strides=1, padding='same', use_bias=False)\n","        self.norm = layers.BatchNormalization()\n","        self.activation = layers.LeakyReLU(0.2)\n","    \n","    def call(self, inputs, training=False):\n","        x = self.conv1(inputs, training=training)\n","        x = self.conv2(x, training=training)\n","        x = self.conv3(x)\n","        x = self.norm(x, training=training)\n","        return self.activation(x + inputs)\n","\n","# Simplified Downsampling Block\n","class DownsampleBlock(layers.Layer):\n","    def __init__(self, filters, **kwargs):\n","        super(DownsampleBlock, self).__init__(**kwargs)\n","        self.conv = ConvBlock(filters, kernel_size=3, strides=2)\n","    \n","    def call(self, inputs, training=False):\n","        return self.conv(inputs, training=training)\n","\n","# Simplified Upsampling Block\n","class UpsampleBlock(layers.Layer):\n","    def __init__(self, filters, **kwargs):\n","        super(UpsampleBlock, self).__init__(**kwargs)\n","        self.upsample = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n","        self.conv = ConvBlock(filters, kernel_size=3, strides=1)\n","    \n","    def call(self, inputs, training=False):\n","        x = self.upsample(inputs)\n","        return self.conv(x, training=training)\n","\n","# Simplified Gnet\n","class G_net(Model):\n","    def __init__(self, **kwargs):\n","        super(G_net, self).__init__(**kwargs)\n","        self.initial_conv = ConvBlock(32, kernel_size=7, strides=1)\n","        self.conv1 = ConvBlock(64, kernel_size=3, strides=2)\n","        self.conv2 = ConvBlock(64)\n","        self.down1 = DownsampleBlock(128)\n","        self.conv3 = ConvBlock(128)\n","        self.dsconv1 = ConvBlock(128, use_depthwise=True)\n","        self.down2 = DownsampleBlock(256)\n","\n","        # Residual Blocks\n","        self.res_blocks = [ResidualBlock(256, use_depthwise=True) for _ in range(4)]\n","        \n","        self.up1 = UpsampleBlock(128)\n","        self.conv4 = ConvBlock(128)\n","        self.up2 = UpsampleBlock(64)\n","        self.conv5 = ConvBlock(64)\n","        self.final_conv = layers.Conv2D(3, kernel_size=7, strides=1, padding='same', activation='tanh')\n","    \n","    def call(self, inputs, training=False):\n","        x = self.initial_conv(inputs, training=training)\n","        x = self.conv1(x, training=training)\n","        x = self.conv2(x, training=training)\n","        x = self.down1(x, training=training)\n","        x = self.conv3(x, training=training)\n","        x = self.dsconv1(x, training=training)\n","        x = self.down2(x, training=training)\n","\n","        for block in self.res_blocks:\n","            x = block(x, training=training)\n","        \n","        x = self.up1(x, training=training)\n","        x = self.conv4(x, training=training)\n","        x = self.up2(x, training=training)\n","        x = self.conv5(x, training=training)\n","        return self.final_conv(x)\n","\n","\n","# Simplified Discriminator\n","\n","class D_net(Model):\n","    def __init__(self, **kwargs):\n","        super(D_net, self).__init__(**kwargs)\n","        self.model = tf.keras.Sequential([\n","            layers.Conv2D(32, kernel_size=3, strides=1, padding='same'),  # K3,S1,C32\n","            layers.LeakyReLU(),  # Leaky ReLU activation\n","            layers.Conv2D(64, kernel_size=3, strides=2, padding='same'),  # K3,S2,C64\n","            layers.LeakyReLU(),  # Leaky ReLU activation\n","            layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),  # K3,S1,C128\n","            layers.LayerNormalization(),  # Instance Normalization\n","            layers.LeakyReLU(),  # Leaky ReLU activation\n","            layers.Conv2D(256, kernel_size=3, strides=2, padding='same'),  # K3,S2,C256\n","            layers.LayerNormalization(),  # Instance Normalization\n","            layers.LeakyReLU(),  # Leaky ReLU activation\n","            layers.Conv2D(1, kernel_size=3, strides=1, padding='same')  # K3,S1,C1\n","        ])\n","    \n","    def call(self, inputs, training=False):\n","        return self.model(inputs, training=training)\n","\n","\n","# Testing Function\n","def test_models(generator, discriminator, input_shape):\n","    dummy_input = tf.random.normal([1] + list(input_shape))  # Batch size of 1\n","    print(\"Testing Generator...\")\n","    generated_output = generator(dummy_input, training=False)\n","    print(f\"Generator output shape: {generated_output.shape}\")\n","    \n","    print(\"Testing Discriminator...\")\n","    discriminator_output = discriminator(generated_output, training=False)\n","    print(f\"Discriminator output shape: {discriminator_output.shape}\")\n","\n","# Clear previous models from memory\n","tf.keras.backend.clear_session()\n","\n","# Initialize models\n","img_size = (256, 256, 3)  # Example image size\n","generator = G_net()\n","discriminator = D_net()\n","\n","# Test the models with a sample input shape\n","test_models(generator, discriminator, img_size)\n","generator.summary()\n","discriminator.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# -------------------------------\n","# AnimeGANv2 Model Class\n","# -------------------------------\n","import tf2onnx\n","\n","class AnimeGANv2(tf.keras.Model):\n","    def __init__(self, config):\n","        super(AnimeGANv2, self).__init__()\n","        # Configuration parameters\n","        self.model_name = config.get('model_name', 'AnimeGANv2')\n","        self.dataset_name = config['dataset_name']\n","        self.train_photo_path = config['train_photo_path']\n","        self.style_path = config['style_path']\n","        self.smooth_path = config['smooth_path']\n","        self.val_path = config['val_path']\n","        \n","        self.epoch = config['epoch']\n","        self.init_epoch = config['init_epoch']\n","        \n","        self.gan_type = config['gan_type']\n","        self.batch_size = config['batch_size']\n","        self.save_freq = config['save_freq']\n","        \n","        self.init_lr = config['init_lr']\n","        self.g_lr = config['g_lr']\n","        self.d_lr = config['d_lr']\n","        self.ld = config['ld']\n","        \n","        self.tv_weight = config['tv_weight']\n","        self.con_weight = config['con_weight']\n","        self.sty_weight = config['sty_weight']\n","        self.color_weight = config['color_weight']\n","        self.g_adv_weight = config['g_adv_weight']\n","        self.d_adv_weight = config['d_adv_weight']\n","        \n","        self.img_size = config['img_size']\n","        self.img_ch = config['img_ch']\n","        \n","        self.ch = config['ch']\n","        self.n_dis = config['n_dis']\n","        self.sn = config['sn']\n","        \n","        self.checkpoint_dir = config['checkpoint_dir']\n","        self.log_dir = config['log_dir']\n","        self.sample_dir = config['sample_dir']\n","        \n","        self.model_dir = self.get_model_dir(config)\n","        \n","        # Ensure directories exist\n","        check_folder(os.path.join(self.checkpoint_dir, self.model_dir))\n","        check_folder(os.path.join(self.log_dir, self.model_dir))\n","        check_folder(os.path.join(self.sample_dir, self.model_dir))\n","        \n","        # Initialize Models\n","        self.generator = G_net()\n","        self.discriminator = D_net()\n","        \n","        # Initialize VGG for content and style loss\n","        vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=(self.img_size[0], self.img_size[1], 3))\n","        self.vgg = Model(inputs=vgg19.input, outputs=[vgg19.get_layer(name).output for name in ['block4_conv4']])\n","        self.vgg.trainable = False\n","        \n","        # Initialize Loss Layers\n","        self.content_loss_layer = ContentLossLayer()\n","        self.style_loss_layer = StyleLossLayer()\n","        self.total_variation_loss_layer = TotalVariationLossLayer()\n","        \n","        # Initialize Optimizers\n","        self.g_optimizer = optimizers.Adam(learning_rate=self.g_lr, beta_1=0.5, beta_2=0.999)\n","        self.d_optimizer = optimizers.Adam(learning_rate=self.d_lr, beta_1=0.5, beta_2=0.999)\n","        \n","        # Initialize Loss Layers\n","        self.generator_loss_layer = GeneratorLossLayer(loss_type=self.gan_type)\n","        self.discriminator_loss_layer = DiscriminatorLossLayer(loss_type=self.gan_type)\n","        \n","        # Initialize Image Generators\n","        self.real_image_generator = ImageGenerator(self.train_photo_path, self.img_size[0], self.batch_size)\n","        self.anime_image_generator = ImageGenerator(self.style_path, self.img_size[0], self.batch_size)\n","        self.anime_smooth_generator = ImageGenerator(self.smooth_path, self.img_size[0], self.batch_size)\n","        self.dataset_num = max(self.real_image_generator.num_images, \n","                               self.anime_image_generator.num_images, \n","                               self.anime_smooth_generator.num_images)\n","        \n","        # Prepare Checkpoints\n","        self.checkpoint = tf.train.Checkpoint(generator=self.generator,\n","                                              discriminator=self.discriminator,\n","                                              optimizer_g=self.g_optimizer,\n","                                              optimizer_d=self.d_optimizer)\n","        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, \n","                                                               os.path.join(self.checkpoint_dir, self.model_dir), \n","                                                               max_to_keep=self.epoch)\n","        \n","        # Restore latest checkpoint if available\n","        if self.checkpoint_manager.latest_checkpoint:\n","            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n","            print(f\"Restored from {self.checkpoint_manager.latest_checkpoint}\")\n","        else:\n","            print(\"Initializing from scratch.\")\n","        \n","        # Initialize TensorBoard writer\n","        self.summary_writer = tf.summary.create_file_writer(os.path.join(self.log_dir, self.model_dir))\n","        \n","    def get_model_dir(self, config):\n","        \"\"\"Define the model directory based on hyperparameters.\"\"\"\n","        return f\"{self.model_name}_{self.dataset_name}_{self.gan_type}_G{int(self.g_adv_weight)}_D{int(self.d_adv_weight)}_\" \\\n","               f\"C{int(self.con_weight)}_S{int(self.sty_weight)}_Color{int(self.color_weight)}_TV{int(self.tv_weight)}\"\n","\n","    @tf.function\n","    def train_step_fn(self, real, anime, anime_smooth):\n","        \"\"\"Single training step.\"\"\"\n","        with tf.GradientTape(persistent=True) as tape:\n","            # Generate images\n","            generated = self.generator(real, training=True)\n","            \n","            # Discriminator outputs\n","            real_output = self.discriminator(anime, training=True)\n","            anime_gray_output = self.discriminator(anime, training=True)  # Assuming anime_gray is same as anime\n","            fake_output = self.discriminator(generated, training=True)\n","            smooth_output = self.discriminator(anime_smooth, training=True)\n","            \n","            # Compute losses\n","            content_loss, style_loss = self.compute_con_sty_loss(real, generated, anime_gray_output)\n","            color_loss = color_loss_fn(real, generated)\n","            tv_loss = self.tv_weight * self.total_variation_loss_layer(generated)\n","            total_loss = (self.con_weight * content_loss +\n","                          self.sty_weight * style_loss +\n","                          self.color_weight * color_loss +\n","                          tv_loss)\n","            \n","            # Generator adversarial loss\n","            g_adv_loss = self.g_adv_weight * self.generator_loss_layer(fake_output)\n","            generator_loss = total_loss + g_adv_loss\n","            \n","            # Discriminator adversarial loss\n","            d_adv_loss = self.d_adv_weight * self.discriminator_loss_layer([real_output, anime_gray_output, fake_output, smooth_output])\n","            discriminator_loss = d_adv_loss + self.gradient_penalty(anime, generated)\n","        \n","        # Compute gradients\n","        gradients_of_generator = tape.gradient(generator_loss, self.generator.trainable_variables)\n","        gradients_of_discriminator = tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n","        \n","        # Apply gradients\n","        self.g_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n","        self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n","        \n","        return {\"g_loss\": generator_loss, \"d_loss\": discriminator_loss}\n","\n","    def compute_con_sty_loss(self, real, generated, anime_gray):\n","        \"\"\"Compute content and style loss.\"\"\"\n","        real_features = self.vgg(real)\n","        generated_features = self.vgg(generated)\n","        anime_gray_features = self.vgg(anime_gray)\n","        \n","        content_loss = sum(self.content_loss_layer(rf, gf) for rf, gf in zip(real_features, generated_features))\n","        style_loss = sum(self.style_loss_layer(agf, gf) for agf, gf in zip(anime_gray_features, generated_features))\n","        tf.print(\"Content Loss:\", content_loss)\n","        tf.print(\"Style Loss:\", style_loss)\n","        return content_loss, style_loss\n","\n","    def gradient_penalty(self, real, fake):\n","        \"\"\"Compute gradient penalty for WGAN-GP and related losses.\"\"\"\n","        if 'dragan' in self.gan_type:\n","            eps = tf.random.uniform(shape=tf.shape(real), minval=0., maxval=1.)\n","            _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n","            x_std = tf.sqrt(x_var)\n","            fake = real + 0.5 * x_std * eps\n","\n","        alpha = tf.random.uniform(shape=[tf.shape(real)[0], 1, 1, 1], minval=0., maxval=1.)\n","        interpolated = real + alpha * (fake - real)\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            disc_interpolated = self.discriminator(interpolated, training=True)\n","        \n","        grad = gp_tape.gradient(disc_interpolated, [interpolated])[0]\n","        grad_norm = tf.norm(tf.reshape(grad, [tf.shape(real)[0], -1]), axis=1)\n","        \n","        if 'lp' in self.gan_type:\n","            GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n","        elif 'gp' in self.gan_type or self.gan_type == 'dragan':\n","            GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n","        else:\n","            GP = 0.0\n","        \n","        return GP\n","\n","    def save_sample(self, epoch):\n","        \"\"\"Save sample images for visual inspection.\"\"\"\n","        val_files = glob(os.path.join(self.val_path, '*.*'))\n","        save_path = os.path.join(self.sample_dir, self.model_dir, f'{epoch:03d}')\n","        check_folder(save_path)\n","        for i, sample_file in enumerate(val_files):\n","            print(f'Saving sample: {i} {sample_file}')\n","            try:\n","                sample_image, _ = self.real_image_generator.load_image(sample_file)\n","                sample_image = np.expand_dims(sample_image, axis=0)  # Add batch dimension\n","                sample_image = tf.convert_to_tensor(sample_image, dtype=tf.float32)\n","                test_generated = self.generator(sample_image, training=False)\n","                test_generated = tf.squeeze(test_generated, axis=0)\n","                # Convert to [0, 255] and uint8\n","                sample_image = tf.cast((sample_image[0] + 1.0) * 127.5, tf.uint8)\n","                test_generated = tf.cast((test_generated + 1.0) * 127.5, tf.uint8)\n","                # Save images\n","                Image.fromarray(sample_image.numpy()).save(os.path.join(save_path, f'{i:03d}_a.jpg'))\n","                Image.fromarray(test_generated.numpy()).save(os.path.join(save_path, f'{i:03d}_b.jpg'))\n","            except Exception as e:\n","                print(f\"Error saving sample {sample_file}: {e}\")\n","        print(f\"Saved samples for epoch {epoch} at {save_path}\")\n","    def save_onnx_model(self, epoch):\n","        \"\"\"Save the model in ONNX format.\"\"\"\n","        model_path = os.path.join(self.checkpoint_dir, self.model_dir, f'model_epoch_{epoch}.onnx')\n","        # Convert the Keras model to ONNX format\n","        model_proto = tf2onnx.convert.from_keras(self)\n","        with open(model_path, \"wb\") as f:\n","            f.write(model_proto.SerializeToString())\n","        print(f\"Model saved in ONNX format at {model_path}\")\n","        \n","    def train(self):\n","        \"\"\"Train the AnimeGANv2 model.\"\"\"\n","        # Initialize TensorBoard writer\n","        self.summary_writer = tf.summary.create_file_writer(os.path.join(self.log_dir, self.model_dir))\n","        \n","        for epoch in range(1, self.epoch + 1):\n","            start_time = time.time()\n","            print(f\"Starting epoch {epoch}/{self.epoch}\")\n","            \n","            init_mean_loss = []\n","            mean_loss = []\n","            for idx in range(int(self.dataset_num / self.batch_size)):\n","                # Load batch data\n","                batch = self.real_image_generator.load_images()\n","                real = batch[:,0,:,:,:]\n","                anime = self.anime_image_generator.load_images()[:,0,:,:,:]\n","                anime_smooth = self.anime_smooth_generator.load_images()[:,0,:,:,:]\n","\n","                # Training phase\n","                if epoch <= self.init_epoch:\n","                    # Pre-training Generator\n","                    with tf.GradientTape() as tape:\n","                        generated = self.generator(real, training=True)\n","#                         self.generator.summary()\n","                        content_loss, style_loss = self.compute_con_sty_loss(real, generated, anime)\n","                        color_loss = color_loss_fn(real, generated)\n","                        tv_loss = self.tv_weight * self.total_variation_loss_layer(generated)\n","                        total_loss = (self.con_weight * content_loss +\n","                                      self.sty_weight * style_loss +\n","                                      self.color_weight * color_loss +\n","                                      tv_loss)\n","                    gradients = tape.gradient(total_loss, self.generator.trainable_variables)\n","                    \n","                    init_mean_loss.append(total_loss)\n","                    # Kiểm tra nếu cả gradient và biến trainable đều có giá trị\n","                    if gradients and self.generator.trainable_variables:\n","                        self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n","                    else:\n","                        print(\"Error: Either gradients or trainable variables are empty!\")\n","                    if (idx + 1) % 200 == 0:\n","                        print(f\"Epoch: {epoch} Step: {idx}/{int(self.dataset_num / self.batch_size)} \"\n","                              f\"Time: {time.time() - start_time:.2f}s Init_Loss: {total_loss.numpy():.8f} \"\n","                              f\"Mean_Init_Loss: {np.mean(init_mean_loss).item():.8f}\")\n","                        init_mean_loss.clear()\n","                else:\n","                    # Update Discriminator\n","                    with tf.GradientTape() as tape:\n","                        generated = self.generator(real, training=True)\n","                        real_output = self.discriminator(anime, training=True)\n","                        anime_gray_output = self.discriminator(anime, training=True)\n","                        fake_output = self.discriminator(generated, training=True)\n","                        smooth_output = self.discriminator(anime_smooth, training=True)\n","                        \n","                        # Compute losses\n","                        d_loss = self.discriminator_loss_layer([real_output, anime_gray_output, fake_output, smooth_output]) + self.gradient_penalty(anime, generated)\n","                    gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","                    self.d_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n","\n","                    # Update Generator\n","                    with tf.GradientTape() as tape:\n","                        generated = self.generator(real, training=True)\n","                        fake_output = self.discriminator(generated, training=True)\n","                        content_loss, style_loss = self.compute_con_sty_loss(real, generated, anime)\n","                        color_loss = color_loss_fn(real, generated)\n","                        tv_loss = self.tv_weight * self.total_variation_loss_layer(generated)\n","                        g_adv_loss = self.g_adv_weight * self.generator_loss_layer(fake_output)\n","                        g_loss = (self.con_weight * content_loss +\n","                                  self.sty_weight * style_loss +\n","                                  self.color_weight * color_loss +\n","                                  tv_loss +\n","                                  g_adv_loss)\n","                    gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n","                    self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n","                    mean_loss.append([d_loss.numpy(),g_loss.numpy()])\n","\n","                    if (idx + 1) % 200 == 0:\n","                        print(f\"Epoch: {epoch} Step: {idx}/{int(self.dataset_num / self.batch_size)} \"\n","                              f\"Time: {time.time() - start_time:.2f}s D_Loss: {d_loss.numpy():.8f}, G_Loss: {g_loss.numpy():.8f} \"\n","                              f\"Mean_dLoss: {np.mean(mean_loss,axis=0)[0]:.8f}\"\n","                              f\"Mean_gLoss: {np.mean(mean_loss,axis=0)[1]:.8f}\")\n","                        mean_loss.clear()\n","\n","            # Save checkpoints\n","            if (epoch % self.save_freq == 0):\n","                self.checkpoint_manager.save()\n","                print(f\"Checkpoint saved at epoch {epoch}\")\n","            \n","            # Save sample images\n","            self.save_sample(epoch)\n","            self.save_onnx_model(epoch)\n","            \n","            print(f\"Epoch {epoch} completed in {time.time() - start_time:.2f}s\")\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# -------------------------------\n","# Main Function\n","# -------------------------------\n","\n","def main():\n","    \"\"\"Main function to train AnimeGANv2.\"\"\"\n","    \n","    # Enable mixed precision if desired (optional)\n","    # mixed_precision.set_global_policy('mixed_float16')\n","    \n","    # Set memory growth option for GPU in TensorFlow 2.x\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if physical_devices:\n","        try:\n","            for device in physical_devices:\n","                tf.config.experimental.set_memory_growth(device, True)\n","            print(\"GPU memory growth set successfully.\")\n","        except RuntimeError as e:\n","            print(f\"Error setting GPU memory growth: {e}\")\n","    \n","    # Configuration dictionary\n","    config = {\n","        'model_name': 'AnimeGANv2',\n","        'dataset_name': 'Hayao',  # Change as needed\n","        'train_photo_path': '/kaggle/input/animegan/train_photo',\n","        'style_path': '/kaggle/input/animegan/Hayao/style',\n","        'smooth_path': '/kaggle/input/animegan/Hayao/smooth',\n","        'val_path': '/kaggle/input/animegan/val',\n","        'epoch': 101,\n","        'init_epoch': 20,\n","        'gan_type': 'lsgan',  # Options: 'gan', 'lsgan', 'wgan-gp', etc.\n","        'batch_size': 12,\n","        'save_freq': 1,\n","        'init_lr': 0.0001,\n","        'g_lr': 0.00001,\n","        'd_lr': 0.00001,\n","        'ld': 10.0,  # Gradient penalty lambda\n","        'tv_weight': 1.0,  # Total Variation Loss weight\n","        'con_weight': 1.5,  # Content Loss weight\n","        'sty_weight': 3.0,  # Style Loss weight\n","        'color_weight': 10.0,  # Color Loss weight\n","        'g_adv_weight': 300.0,  # Adversarial Loss weight for Generator\n","        'd_adv_weight': 300.0,  # Adversarial Loss weight for Discriminator\n","        'img_size': [256, 256],\n","        'img_ch': 3,\n","        'ch': 64,  # Base channel number per layer\n","        'n_dis': 3,  # Number of discriminator layers\n","        'sn': True,  # Use spectral normalization\n","        'checkpoint_dir': 'checkpoint',\n","        'log_dir': 'logs',\n","        'sample_dir': 'samples'\n","    }\n","    \n","    # Initialize and train the model\n","    gan = AnimeGANv2(config)\n","    gan.train()\n","    print(\" [*] Training finished!\")\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1218973,"sourceId":2035659,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
